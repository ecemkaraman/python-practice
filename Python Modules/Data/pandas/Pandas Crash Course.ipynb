{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd0eb5a",
   "metadata": {},
   "source": [
    "# Mastering the Pandas Library\n",
    "This notebook covers detailed topics related to the Pandas library in Python, following a structured outline for practical use cases and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1387a7f2",
   "metadata": {},
   "source": [
    "## 1. Basics and Fundamentals\n",
    "- Series: 1D & homogeneous → container for scalars - aka a column\n",
    "    - If you create a Series from a dict, the keys will then become the labels. \n",
    "- DataFrame: 2D & heterogeneous → contains a collection of *series*\n",
    "    - Can be created from dictionaries, lists, NumPy arrays, or external files.\n",
    "    - With the `index` argument, you can name your own indexes for Series or row titles for DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d36b13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    10\n",
      "1    20\n",
      "2    30\n",
      "Name: Sample Series, dtype: int64\n",
      "a    10\n",
      "b    20\n",
      "c    30\n",
      "d    40\n",
      "dtype: int64\n",
      "      Name  Age\n",
      "0    Alice   25\n",
      "1      Bob   30\n",
      "2  Charlie   35\n",
      "# First 5 rows:\n",
      "      Name  Age\n",
      "0    Alice   25\n",
      "1      Bob   30\n",
      "2  Charlie   35\n",
      "\n",
      "\n",
      "Quick One-shot Summary\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Name    3 non-null      object\n",
      " 1   Age     3 non-null      int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 180.0+ bytes\n",
      "# Summary of numerical columns(mean, count, std, min, max):\n",
      "        Age\n",
      "count   3.0\n",
      "mean   30.0\n",
      "std     5.0\n",
      "min    25.0\n",
      "25%    27.5\n",
      "50%    30.0\n",
      "75%    32.5\n",
      "max    35.0\n",
      "\n",
      "# (row count, column count): (3, 2)\n",
      "\n",
      "# Column Data types:\n",
      "Name    object\n",
      "Age      int64\n",
      "dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Creating a Series\n",
    "series = pd.Series([10, 20, 30], name='Sample Series')\n",
    "print(series)\n",
    "\n",
    "# Create a Series with custom labelled index\n",
    "data = [10, 20, 30, 40]\n",
    "labelled_series = pd.Series(data, index=[\"a\", \"b\", \"c\", \"d\"])\n",
    "print(labelled_series)\n",
    "\n",
    "# Creating a DataFrame\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35]}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "# Inspecting a DataFrame\n",
    "print(f\"# First 5 rows:\\n{df.head()}\\n\") # tail()=last x rows\n",
    "print(\"\\nQuick One-shot Summary\")\n",
    "df.info() #directly outputs to console, can't do print(df.info())\n",
    "print(f\"# Summary of numerical columns(mean, count, std, min, max):\\n{df.describe()}\\n\")\n",
    "print(f\"# (row count, column count): {df.shape}\\n\")\n",
    "print(f\"# Column Data types:\\n{df.dtypes}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f79e51",
   "metadata": {},
   "source": [
    "## 2. Data I/O: Read/Write Files of Various Formats\n",
    "**CSV**: `pd.read_csv()`, `DataFrame.to_csv()`\n",
    " - `index=False` tells Pandas **not to include the row index** in the exported file. By default, the DF row index is the 1st column in the output CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271f5212",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('input.csv', index=False) #write\n",
    "csv_data = pd.read_csv('output.csv') #read\n",
    "print(csv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0907d663",
   "metadata": {},
   "source": [
    "**JSON**: `pd.read_json()` / `.to_json()`\n",
    "- **`orient`**: Specifies the JSON format to control how data is structured during conversion between DF/Series and JSON. Possible values:\n",
    "    - **`records`**: For row-based processing or compatibility with APIs.\n",
    "    - **`columns`**: Default; useful for column-wise data storage.\n",
    "    - **`index`**: For row-wise lookup based on indices.\n",
    "    - **`split`**: Ideal for recreating DataFrame structure programmatically.\n",
    "    - **`table`**: For interoperability with table-oriented frameworks or tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0edab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and writing JSON files\n",
    "df.to_json('example.json', orient='records')\n",
    "json_data = pd.read_json('example.json')\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ecbbbd",
   "metadata": {},
   "source": [
    "**Excel**: `pd.read_excel()`, `DataFrame.to_excel()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5426e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('example.xlsx', sheet_name='Sheet1')\n",
    "df.to_excel('output.xlsx', sheet_name='Sheet1', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6bdd73",
   "metadata": {},
   "source": [
    "**SQL**: `pd.read_sql()` / `.to_sql()` \n",
    "\n",
    "- **`pd.read_sql`**: Reads data into a DF from either an **entire table or a SQL query**, offering flexibility.\n",
    "- **`pd.read_sql_query`**: **Executes SQL queries** and loads the result into a DF. **Only accepts SQL queries** as input.\n",
    "- **SQL Connection**:A DB connection (e.g., using `sqlalchemy.create_engine`) is required for both reading and writing.\n",
    "     - `con=engine`=DB connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8b2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///:memory:') #Connect to DB\n",
    "# read_sql()\n",
    "df = pd.read_sql('SELECT * FROM table_name', con=engine) #Read via query\n",
    "df = pd.read_sql('table_name', con=engine)  # Reads entire table\n",
    "\n",
    "# read_sql_query()\n",
    "df = pd.read_sql_query('SELECT * FROM table_name', con=engine)\n",
    "\n",
    "# Write to SQL DB\n",
    "df.to_sql('output_table', engine)\n",
    "df.to_sql('output_table', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16aec78",
   "metadata": {},
   "source": [
    "**Parquet**: `pd.read_parquet()` / `.to_parquet()`\n",
    "- **Parquet** is a **columnar storage file format** that is optimized for efficient data storage and retrieval, commonly used in big data processing frameworks like Apache Spark, Hadoop, and Pandas. It is designed to handle large datasets efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f816b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('example.parquet')\n",
    "df.to_parquet('example.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec18c9",
   "metadata": {},
   "source": [
    "**Handling Large Files**:Chunk-based reading (`chunksize`)->Specifies the number of rows to read at a time. \n",
    "- UC: File types with tabular data and are processed row-by-row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6e0fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"large_dataset.csv\", \"w\") as f:\n",
    "    f.write(csv_data)\n",
    "\n",
    "# Read the CSV file in chunks\n",
    "chunk_size = 2  # Number of rows per chunk\n",
    "chunk_iterator = pd.read_csv(\"large_dataset.csv\", chunksize=chunk_size)\n",
    "\n",
    "# Process each chunk\n",
    "for chunk in chunk_iterator:\n",
    "    print(f\"Processing chunk:\\n{chunk}\\n\")\n",
    "    # Example: Perform operations like filtering\n",
    "    filtered_chunk = chunk[chunk['Age'] > 30]\n",
    "    print(f\"Filtered chunk:\\n{filtered_chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee9e3f7",
   "metadata": {},
   "source": [
    "## 3. Indexing and Selection\n",
    "Access, subset and filter large datasets for specific rows/columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c7faa2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Name  Age      City\n",
      "row1    Alice   25  New York\n",
      "row2      Bob   30    London\n",
      "row3  Charlie   35     Paris\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'City': ['New York', 'London', 'Paris']\n",
    "}, index=['row1', 'row2', 'row3'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195fb77d",
   "metadata": {},
   "source": [
    "1. **Column Selection**:\n",
    "    - Single column: `df['column_name']`\n",
    "    - Multiple columns: `df[['col1', 'col2']]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d95063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-Based Indexing\n",
    "print(\"Single column (by label):\\n\", df['Name'])  \n",
    "print(\"Multiple columns:\\n\", df[['Name', 'City']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd341170",
   "metadata": {},
   "source": [
    "2. **Row Selection**: Return specified row(s) as a Series\n",
    "    - By label: `df.loc['row_label']`\n",
    "    - By position(int): `df.iloc[row_index]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51239636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single row (by label):\n",
      " Name       Alice\n",
      "Age           25\n",
      "City    New York\n",
      "Name: row1, dtype: object\n",
      "Multiple rows:\n",
      "          Name  Age      City\n",
      "row1    Alice   25  New York\n",
      "row3  Charlie   35     Paris\n",
      "Specific value (by label): London\n"
     ]
    }
   ],
   "source": [
    "# Row-Based Indexing with `loc`\n",
    "print(\"\\nSingle row (by label):\\n\", df.loc['row1'])  \n",
    "print(\"Multiple rows:\\n\", df.loc[['row1', 'row3']])  \n",
    "print(\"Specific value (by label):\", df.loc['row2', 'City']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36d8b342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single row (by position):\n",
      " Name       Alice\n",
      "Age           25\n",
      "City    New York\n",
      "Name: row1, dtype: object\n",
      "Multiple rows:\n",
      "          Name  Age      City\n",
      "row1    Alice   25  New York\n",
      "row3  Charlie   35     Paris\n",
      "Specific value (by position): London\n"
     ]
    }
   ],
   "source": [
    "# Row-Based Indexing with `iloc`\n",
    "print(\"\\nSingle row (by position):\\n\", df.iloc[0])  \n",
    "print(\"Multiple rows:\\n\", df.iloc[[0, 2]])  \n",
    "print(\"Specific value (by position):\", df.iloc[1, 2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b3c9b0",
   "metadata": {},
   "source": [
    "3. **Specific Cell**:\n",
    "    - By label: `df.loc['row_label', 'column_name']`\n",
    "    - By position: `df.iloc[row_index, col_index]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a95403",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_by_label = df.loc['row2', 'City']  \n",
    "print(\"Cell by label:\", cell_by_label)\n",
    "\n",
    "cell_by_position = df.iloc[1, 2] \n",
    "print(\"Cell by position:\", cell_by_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ddeb19",
   "metadata": {},
   "source": [
    "4. **Slicing**:\n",
    "    - Rows: `df[2:5]` \n",
    "    - Rows/columns by labels(inclusive): `df.loc[start_label:end_label]`\n",
    "    - Rows/columns by indices(exclusive): `df.iloc[start:end]` \n",
    "- **Label slicing includes the end label, index slicing excludes the end index!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52e67307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing rows (df[1:3]):\n",
      "          Name  Age    City\n",
      "row2      Bob   30  London\n",
      "row3  Charlie   35   Paris\n",
      "\n",
      "Slicing by labels:\n",
      "        Name  Age\n",
      "row1  Alice   25\n",
      "row2    Bob   30\n",
      "\n",
      "Slicing by indices:\n",
      "        Name  Age\n",
      "row1  Alice   25\n",
      "row2    Bob   30\n"
     ]
    }
   ],
   "source": [
    "row_slice = df[1:3]  # Slicing by row indices - exclusive \n",
    "print(\"Slicing rows (df[1:3]):\\n\", row_slice)\n",
    "\n",
    "# Slicing Rows/Columns by Labels - inclusive\n",
    "label_slice = df.loc['row1':'row2', 'Name':'Age']  \n",
    "print(\"\\nSlicing by labels:\\n\", label_slice)\n",
    "\n",
    "# Slicing Rows/Columns by Indices\n",
    "index_slice = df.iloc[0:2, 0:2]  \n",
    "print(\"\\nSlicing by indices:\\n\", index_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf1d23f",
   "metadata": {},
   "source": [
    "5. **Conditional/Boolean Indexing**:\n",
    "- **Filter rows**: `df[df['column_name'] > value]`\n",
    "    - Use **`df[...]`** for basic filtering.\n",
    "- **Filter rows with `loc`**: `df.loc[df['column_name'] == 'value']`\n",
    "    - Use **`df.loc[...]`** for filtering + selecting specific columns (label-based) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "320af526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df[...] example:\n",
      "          Name  Age   City\n",
      "row3  Charlie   35  Paris\n",
      "         Name  Age   City\n",
      "row3  Charlie   35  Paris\n",
      "\n",
      "df.loc[...] example:\n",
      "          Name   City\n",
      "row3  Charlie  Paris\n"
     ]
    }
   ],
   "source": [
    "# Filter rows (boolean) with df[...]\n",
    "filtered_simple = df[df['Age'] > 30]\n",
    "print(\"df[...] example:\\n\", filtered_simple)\n",
    "\n",
    "# Multiple conditions\n",
    "print(df[(df['Age'] > 25) & (df[\"Name\"] == 'Charlie')]) \n",
    "\n",
    "# Filter rows (conditional) with df.loc[...]\n",
    "filtered_advanced = df.loc[df['Age'] > 30, ['Name', 'City']]\n",
    "print(\"\\ndf.loc[...] example:\\n\", filtered_advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7135bebe",
   "metadata": {},
   "source": [
    "6. **Advanced Indexing**:\n",
    "    - Select specific rows and columns: `df.loc[[row1, row2], [col1, col2]]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a19c67",
   "metadata": {},
   "source": [
    "**Setting and Resetting Index**:\n",
    " - `.set_index()`: Create a MultiIndex for hierarchical organization.\n",
    " - `.reset_index()`: Flatten the index back into regular columns.\n",
    " \n",
    " Set MultiIndex: set_index() creates MultiIndex \n",
    "Reset MultiIndex: reset_index() flattens the index back into regular columns.\n",
    "- **Single-Level vs Multi-Level Data**: Single-level data uses a flat index for rows/columns, while multi-level (or hierarchical) data uses multiple index levels to represent relationships or groupings within the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8986fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting and resetting an index\n",
    "df.set_index('Name', inplace=True)\n",
    "print(df)\n",
    "df.reset_index(inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c89e0e",
   "metadata": {},
   "source": [
    "**MultiIndex (hierarchical indexing)**\n",
    "- **MultiIndex** is a hierarchical index structure->manage multi-D data by using multiple levels of row or column indices.\n",
    "- **How to Create**: \n",
    "    - `set_index()` is the most common way to create a MultiIndex by promoting one or more columns to row indices.\n",
    "    - Alternative: `pd.MultiIndex.from_tuples()`/`.from_product()`->more flexible\n",
    "\n",
    "- `reset_index()` is used to revert the MultiIndex into columns.\n",
    "\n",
    "- **Why Use It**: It organizes and analyzes grouped or hierarchical datasets more efficiently, enabling operations like slicing, aggregation, and pivoting on multiple levels.\n",
    "Use `pd.MultiIndex.from_tuples()` or `set_index()` to define multiple index levels, enabling hierarchical access and manipulation of data.\n",
    "- **`pd.IndexSlice`** is better for slicing when working with **MultiIndex** because it provides a clean and flexible way to specify slices across multiple levels of the index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9a314a",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning and Preprocessing\n",
    "Clean raw data by filling missing values, converting types, and removing duplicates.\n",
    "- **Handling Missing Data**:\n",
    "    - `.isna()`\n",
    "    - `.fillna()`\n",
    "    - `.dropna()`\n",
    "    - `.interpolate()`: Fills missing values by estimating them using neighboring values.\n",
    "- For nun-numeric data, it's ok to use **`None`** for missing values, but BP to use **`np.nan`** for numerical or mixed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5b47d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  4.0\n",
      "1  NaN  5.0\n",
      "2  3.0  NaN\n",
      "Missing values:\n",
      "        A      B\n",
      "0  False  False\n",
      "1   True  False\n",
      "2  False   True\n",
      "Total Number of Missing Values:\n",
      " A    1\n",
      "B    1\n",
      "dtype: int64\n",
      "\n",
      "After filling missing values with 0:\n",
      "      A    B\n",
      "0  1.0  4.0\n",
      "1  0.0  5.0\n",
      "2  3.0  0.0\n",
      "\n",
      "After dropping rows with missing values:\n",
      "      A    B\n",
      "0  1.0  4.0\n",
      "\n",
      "After interpolating missing values:\n",
      "      A    B\n",
      "0  1.0  4.0\n",
      "1  2.0  5.0\n",
      "2  3.0  5.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'A': [1, np.nan, 3], 'B': [4, 5, np.nan]})\n",
    "print(df)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values:\\n\", df.isna())\n",
    "print(\"Total Number of Missing Values:\\n\", df.isna().sum())\n",
    "\n",
    "# Fill missing values\n",
    "df_filled = df.fillna(0)\n",
    "print(\"\\nAfter filling missing values with 0:\\n\", df_filled)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_dropped = df.dropna()\n",
    "print(\"\\nAfter dropping rows with missing values:\\n\", df_dropped)\n",
    "\n",
    "# Interpolate missing values\n",
    "df_interpolated = df.interpolate()\n",
    "print(\"\\nAfter interpolating missing values:\\n\", df_interpolated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c0081f",
   "metadata": {},
   "source": [
    "- **Data Type Conversion**: `.astype()`\n",
    "- **String Operations**:`.str` methods for cleaning and manipulating text data. (.upper(), .strip(), .replace(), .contains(), .len())\n",
    "- **Duplicated Data**:\n",
    "    - **`.duplicated()`**: Flags duplicate rows.\n",
    "    - **`.drop_duplicates()`**: Removes duplicates, keeping the first occurrence by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ba77b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name     object\n",
      "Age     float64\n",
      "dtype: object\n",
      "      Name   Age\n",
      "0    ALICE  25.0\n",
      "1      BOB  30.0\n",
      "2    ALICE  25.0\n",
      "3  CHARLIE  35.0\n",
      "4      BOB  30.0\n",
      "Duplicated rows:\n",
      " 0    False\n",
      "1    False\n",
      "2     True\n",
      "3    False\n",
      "4     True\n",
      "dtype: bool\n",
      "\n",
      "DataFrame after dropping duplicates:\n",
      "       Name   Age\n",
      "0    ALICE  25.0\n",
      "1      BOB  30.0\n",
      "3  CHARLIE  35.0\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob'],\n",
    "    'Age': [25, 30, 25, 35, 30]\n",
    "})\n",
    "\n",
    "# Converting data types\n",
    "df['Age'] = df['Age'].astype(float)\n",
    "print(df.dtypes)\n",
    "\n",
    "# String operations\n",
    "df['Name'] = df['Name'].str.upper()\n",
    "print(df)\n",
    "\n",
    "# Check for duplicated rows\n",
    "duplicates = df.duplicated()\n",
    "print(\"Duplicated rows:\\n\", duplicates)\n",
    "\n",
    "# Drop duplicate rows\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "print(\"\\nDataFrame after dropping duplicates:\\n\", df_no_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f652e",
   "metadata": {},
   "source": [
    "## 5. Data Transformation\n",
    "Transform datasets into desired shapes or formats for analysis.\n",
    "- **Column Operations**:\n",
    "    - Add, remove, rename, and reorder columns.\n",
    "- **Apply and Map Functions**:\n",
    "    - `.apply()`, `.map()`, `.applymap()` for custom transformations.\n",
    "- **Sorting**:\n",
    "    - `.sort_values()` / `.sort_index()`.\n",
    "- **Pivot Tables**:\n",
    "    - `.pivot_table()` for aggregations and summaries.\n",
    "- **Melt and Reshape**:\n",
    "    - `.melt()`, `.stack()`, `.unstack()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84daee32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Name  Age      City\n",
      "row1    Alice   25  New York\n",
      "row2      Bob   30    London\n",
      "row3  Charlie   35     Paris\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'City': ['New York', 'London', 'Paris']\n",
    "}, index=['row1', 'row2', 'row3'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05ea1368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added a Salary column:\n",
      "          Name  Age      City  Net_income  Salary\n",
      "row1    Alice   25  New York     40000.0   50000\n",
      "row2      Bob   30    London     46200.0   60000\n",
      "row3  Charlie   35     Paris     53900.0   70000\n",
      "\n",
      "Increase Salary by 10%:\n",
      "          Name  Age      City  Net_income   Salary\n",
      "row1    Alice   25  New York     40000.0  55000.0\n",
      "row2      Bob   30    London     46200.0  66000.0\n",
      "row3  Charlie   35     Paris     53900.0  77000.0\n",
      "\n",
      "Renamed Salary column to Income:\n",
      "          Name  Age      City  Net_income   Income\n",
      "row1    Alice   25  New York     40000.0  55000.0\n",
      "row2      Bob   30    London     46200.0  66000.0\n",
      "row3  Charlie   35     Paris     53900.0  77000.0\n",
      "\n",
      " Create Net_income column, applied a lambda function:\n",
      "          Name  Age      City  Net_income   Income\n",
      "row1    Alice   25  New York     38500.0  55000.0\n",
      "row2      Bob   30    London     46200.0  66000.0\n",
      "row3  Charlie   35     Paris     53900.0  77000.0\n",
      "\n",
      "Pivot table (Mean income by Age):\n",
      "       Income\n",
      "Age         \n",
      "25   55000.0\n",
      "30   66000.0\n",
      "35   77000.0\n",
      "\n",
      "Deleted Income column:\n",
      "          Name  Age      City  Net_income\n",
      "row1    Alice   25  New York     38500.0\n",
      "row2      Bob   30    London     46200.0\n",
      "row3  Charlie   35     Paris     53900.0\n",
      "\n",
      "Replace Alice's Net Income:\n",
      "          Name  Age      City  Net_income\n",
      "row1    Alice   25  New York     40000.0\n",
      "row2      Bob   30    London     46200.0\n",
      "row3  Charlie   35     Paris     53900.0\n"
     ]
    }
   ],
   "source": [
    "df['Salary'] = [50000, 60000, 70000] \n",
    "print(\"Added a Salary column:\\n\", df)\n",
    "\n",
    "df['Salary'] = df['Salary'] * 1.1\n",
    "print(\"\\nIncrease Salary by 10%:\\n\", df)\n",
    "\n",
    "df.rename(columns={'Salary': 'Income'}, inplace=True) \n",
    "print(\"\\nRenamed Salary column to Income:\\n\", df)\n",
    "\n",
    "df['Net_income'] = df['Income'].apply(lambda x: x * 0.7)\n",
    "print(\"\\n Create Net_income column, applied a lambda function:\\n\", df)\n",
    "\n",
    "pivot = pd.pivot_table(df, values='Income', index='Age', aggfunc='mean')\n",
    "print(\"\\nPivot table (Mean income by Age):\\n\", pivot)\n",
    "\n",
    "df = df.drop(columns=['Income'])\n",
    "print(\"\\nDeleted Income column:\\n\", df)\n",
    "\n",
    "df['Net_income'].replace(38500.0, 40000.0, inplace=True)  # Replace \n",
    "print(\"\\nReplace Alice's Net Income:\\n\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a4c81b",
   "metadata": {},
   "source": [
    "## 6. Aggregation and Grouping\n",
    "**GroupBy Operations**: Group data by a column and calculate aggregate statistics.\n",
    " - `.groupby()` for aggregation (`sum`, `mean`, `count`, etc.).\n",
    " - **`.groupby('Col1', 'Col2').agg()`** applies multiple aggregation functions (sum, mean, min, max) on different columns (Col1, Col2) in a single operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d45921e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Category  Value1  Value2\n",
      "0        A      10       5\n",
      "1        A      20      15\n",
      "2        B      30      25\n",
      "3        B      40      35\n",
      "4        A      50      45\n",
      "5        B      60      55\n",
      "GroupBy Sum:\n",
      "           Value1  Value2\n",
      "Category                \n",
      "A             80      65\n",
      "B            130     115\n",
      "\n",
      "GroupBy Count:\n",
      "           Value1  Value2\n",
      "Category                \n",
      "A              3       3\n",
      "B              3       3\n",
      "GroupBy Aggregation using .agg():\n",
      "          Value1            Value2    \n",
      "            sum       mean    min max\n",
      "Category                             \n",
      "A            80  26.666667      5  45\n",
      "B           130  43.333333     25  55\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Category': ['A', 'A', 'B', 'B', 'A', 'B'],\n",
    "    'Value1': [10, 20, 30, 40, 50, 60],\n",
    "    'Value2': [5, 15, 25, 35, 45, 55]\n",
    "})\n",
    "print(df)\n",
    "\n",
    "# Group by 'Category' and calculate sum of each column\n",
    "grouped_sum = df.groupby('Category').sum()\n",
    "print(\"GroupBy Sum:\\n\", grouped_sum)\n",
    "\n",
    "# Group by 'Category' and count non-null entries in each column\n",
    "grouped_count = df.groupby('Category').count()\n",
    "print(\"\\nGroupBy Count:\\n\", grouped_count)\n",
    "\n",
    "# Group by 'Category' and calculate multiple aggregation statistics\n",
    "grouped = df.groupby('Category').agg({\n",
    "    'Value1': ['sum', 'mean'],    # Sum and mean of Value1\n",
    "    'Value2': ['min', 'max']      # Min and max of Value2\n",
    "})\n",
    "print(\"GroupBy Aggregation using .agg():\\n\", grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3191855",
   "metadata": {},
   "source": [
    "**Window and Rolling Operations**: Apply rolling metrics on time-series data \n",
    "- **`set_index()`**: Sets the specified column as the index of the DF\n",
    "- **`rolling(window=3)`**: Creates a rolling window view of the data over a specified window size-> can calculate metrics like `mean()`, `min()`, `max()`,  `std()`,  `var()`,  `sum()` within that window. Can also use `count()`and `apply()` (for custom functions).\n",
    "- **`expanding()`**: Expands the window to include all prior data points for each row, allowing for cumulative calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14ad4231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  Value\n",
      "0 2023-01-01     10\n",
      "1 2023-01-02     20\n",
      "2 2023-01-03     30\n",
      "3 2023-01-04     40\n",
      "4 2023-01-05     50\n",
      "5 2023-01-06     60\n",
      "\n",
      "Rolling and Expanding Operations:\n",
      "             Value  Rolling_Sum  Expanding_Sum\n",
      "Date                                         \n",
      "2023-01-01     10          NaN           10.0\n",
      "2023-01-02     20          NaN           30.0\n",
      "2023-01-03     30         60.0           60.0\n",
      "2023-01-04     40         90.0          100.0\n",
      "2023-01-05     50        120.0          150.0\n",
      "2023-01-06     60        150.0          210.0\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Date': pd.date_range('2023-01-01', periods=6, freq='D'),\n",
    "    'Value': [10, 20, 30, 40, 50, 60]\n",
    "})\n",
    "print(df)\n",
    "\n",
    "# Set Date as index\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Apply rolling window operation (e.g., 3-day rolling sum)\n",
    "df['Rolling_Sum'] = df['Value'].rolling(window=3).sum()\n",
    "\n",
    "# Apply expanding window operation (cumulative sum)\n",
    "df['Expanding_Sum'] = df['Value'].expanding().sum()\n",
    "\n",
    "print(\"\\nRolling and Expanding Operations:\\n\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bbf20c",
   "metadata": {},
   "source": [
    "## 7. Merging and Combining Data\n",
    "Combine data from multiple sources into a single coherent dataset.\n",
    "- **Merge and Join**:\n",
    "    - `.merge()` (similar to SQL joins).\n",
    "    - `.join()` for index-based merging.\n",
    "- **Concatenation**:\n",
    "    - `pd.concat()` to combine along a specific axis.\n",
    "- **Database-like Operations**:\n",
    "    - Append, union, intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a3c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two DataFrames\n",
    "df1 = pd.DataFrame({'ID': [1, 2], 'Name': ['Alice', 'Bob']})\n",
    "df2 = pd.DataFrame({'ID': [2, 3], 'City': ['London', 'Paris']})\n",
    "\n",
    "# Merge DataFrames\n",
    "merged = pd.merge(df1, df2, on='ID', how='outer')\n",
    "print(merged)\n",
    "\n",
    "\n",
    "# Concatenating DataFrames\n",
    "df3 = pd.DataFrame({'Name': ['Eve'], 'Age': [40], 'Income': [80000]})\n",
    "concat = pd.concat([df, df3], ignore_index=True)\n",
    "print(concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e3738",
   "metadata": {},
   "source": [
    "## 8. Time-Series Data\n",
    "Handle and analyze temporal data efficiently.\n",
    "- **Datetime Handling**:\n",
    "    - `pd.to_datetime()`, `.dt` accessor for datetime attributes.\n",
    "- **Resampling**:\n",
    "    - `.resample()` for up/down sampling of time-series data.\n",
    "- **Shifting and Lagging**:\n",
    "    - `.shift()` for lag analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2756915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a time-series DataFrame\n",
    "time_data = pd.date_range(start='2023-01-01', periods=5, freq='D')\n",
    "time_df = pd.DataFrame({'Date': time_data, 'Value': range(5)})\n",
    "time_df.set_index('Date', inplace=True)\n",
    "print(time_df)\n",
    "\n",
    "# Resampling data\n",
    "resampled = time_df.resample('2D').sum()\n",
    "print(resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d6045d",
   "metadata": {},
   "source": [
    "## 9. Performance Optimization\n",
    "Optimize memory and computational performance.\n",
    "- **Memory Optimization**:\n",
    "    - Reducing data size by converting types (`pd.to_numeric()` with `downcast`).\n",
    "- **Vectorization**:\n",
    "    - Leveraging NumPy-backed operations over Python loops.\n",
    "- **Efficient I/O**:\n",
    "    - Use binary formats like Parquet or Feather for faster reads/writes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c0b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing memory usage by converting data types\n",
    "optimized_df = df.copy()\n",
    "optimized_df['Income'] = pd.to_numeric(optimized_df['Income'], downcast='integer')\n",
    "print(optimized_df.dtypes)\n",
    "\n",
    "# Using vectorized operations\n",
    "df['Adjusted_Income'] = df['Income'] * 0.9\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9aafd0",
   "metadata": {},
   "source": [
    "## 10. Visualization\n",
    "Quickly visualize data trends and distributions.\n",
    "- **Basic Plotting**:\n",
    "    - `.plot()` for quick visualizations.\n",
    "- **Integration with Libraries**:\n",
    "    - Seamless use with `matplotlib`, `seaborn`, and `plotly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3771d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic plotting\n",
    "df['Income'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a578d30e",
   "metadata": {},
   "source": [
    "## 11. Advanced Topics\n",
    "Master advanced operations like MultiIndex, sparse matrices, custom aggregations, and handling hierarchical data.\n",
    "- **Custom Aggregations**:\n",
    "    - Using `.agg()` with lambda functions or multiple aggregations per column.\n",
    "- **MultiIndex Operations**:\n",
    "    - Slicing, swapping levels, and combining MultiIndex DataFrames.\n",
    "- **Categorical Data**:\n",
    "    - Optimizing memory and performance with `.astype('category')`.\n",
    "- **Sparse Data**:\n",
    "    - Handling sparse DataFrames for memory efficiency.\n",
    "- **Working with JSON/Hierarchical Data**:\n",
    "    - Normalizing nested JSON with `pd.json_normalize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiIndex DataFrame\n",
    "multi_index_df = pd.DataFrame({'Category': ['A', 'A', 'B'], 'Subcategory': ['X', 'Y', 'Z'], 'Values': [1, 2, 3]})\n",
    "multi_index_df.set_index(['Category', 'Subcategory'], inplace=True)\n",
    "print(multi_index_df)\n",
    "\n",
    "# Custom aggregations\n",
    "custom_agg = df.agg({'Income': ['sum', 'mean'], 'Age': 'max'})\n",
    "print(custom_agg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
