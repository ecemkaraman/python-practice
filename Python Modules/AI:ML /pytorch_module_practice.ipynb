{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb63a599",
   "metadata": {},
   "source": [
    "# **Python `torch` Module Practice**\n",
    "This notebook provides an overview and practice examples for the PyTorch `torch` module, which is used for tensor computations, automatic differentiation, and building machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292b8b2",
   "metadata": {},
   "source": [
    "## **1. Installing PyTorch**\n",
    "Ensure PyTorch is installed using the appropriate command for your environment. Visit the [PyTorch Installation Guide](https://pytorch.org/get-started/locally/) for details.\n",
    "\n",
    "Import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0b3ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24228c3",
   "metadata": {},
   "source": [
    "## **2. Tensors**\n",
    "Tensors are the fundamental building blocks of PyTorch. They are similar to NumPy arrays but can run on GPUs for faster computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98819a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "y = torch.ones((2, 2))\n",
    "\n",
    "print(f\"Tensor x:\\n{x}\")\n",
    "print(f\"Tensor y:\\n{y}\")\n",
    "\n",
    "# Tensor operations\n",
    "z = x + y\n",
    "print(f\"Tensor addition result:\\n{z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc051f8d",
   "metadata": {},
   "source": [
    "## **3. Automatic Differentiation**\n",
    "PyTorch provides `autograd` for automatic differentiation, which is essential for training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b70b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic differentiation example\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**3\n",
    "y.backward()  # Compute gradients\n",
    "print(f\"Gradient of y with respect to x: {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85113bb6",
   "metadata": {},
   "source": [
    "## **4. Building Neural Networks**\n",
    "Neural networks in PyTorch are built using the `torch.nn` module. Define a network as a subclass of `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11edc08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instantiate the network\n",
    "model = SimpleNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf175d9e",
   "metadata": {},
   "source": [
    "## **5. Loss Functions and Optimizers**\n",
    "Define a loss function and an optimizer to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c4d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ef5f4",
   "metadata": {},
   "source": [
    "## **6. Training Loop**\n",
    "Train the model by iterating through the data, computing loss, and updating weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7777873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data for training\n",
    "inputs = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "targets = torch.tensor([[1.0], [2.0]])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60792cc",
   "metadata": {},
   "source": [
    "## **7. GPU Acceleration**\n",
    "PyTorch supports running computations on GPUs for faster processing. Use `.to('cuda')` to move data and models to a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dfc48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model and data to GPU\n",
    "model = model.to(device)\n",
    "inputs, targets = inputs.to(device), targets.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d3e5d",
   "metadata": {},
   "source": [
    "## **8. Saving and Loading Models**\n",
    "PyTorch provides utilities to save and load model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f03672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model parameters\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "print(\"Model saved.\")\n",
    "\n",
    "# Load model parameters\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e7c5fa",
   "metadata": {},
   "source": [
    "## **9. Practical Example: Logistic Regression**\n",
    "Demonstrating a simple binary classification task using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15396c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "# Instantiate and train the model\n",
    "log_model = LogisticRegression()\n",
    "optimizer = optim.SGD(log_model.parameters(), lr=0.1)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "\n",
    "inputs = torch.tensor([[1.0, 2.0], [3.0, 4.0], [1.5, 2.5]], requires_grad=True)\n",
    "targets = torch.tensor([[0.0], [1.0], [0.0]])\n",
    "\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = log_model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
